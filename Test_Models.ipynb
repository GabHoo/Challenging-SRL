{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabHoo/Challenging-SRL/blob/main/Test_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SETTING UP\n",
        "Run the following cells in the Settin Up section to be able to run any of the tests in this notebook with the two suggested models.  \n",
        "Change the current value for both model path according to your own models location. If No models are found they will be downloaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iDMzdKns-m8m"
      },
      "outputs": [],
      "source": [
        "# INSTALL AND IMPORTS\n",
        "\"\"\"\n",
        "pip install allennlp\n",
        "pip install allennlp-models\n",
        "pip install -U spaCy\n",
        "pip install checklist\n",
        "\"\"\"\n",
        "from allennlp.predictors import Predictor\n",
        "import allennlp_models.tagging\n",
        "\n",
        "import json\n",
        "import os\n",
        "from utils import *\n",
        "import utils\n",
        "import re\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Change the model here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model found!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gabhoo/.local/lib/python3.10/site-packages/spacy/util.py:887: UserWarning: [W095] Model 'en_core_web_sm' (3.3.0) was trained with spaCy v3.3 and may not be 100% compatible with the current version (3.5.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SRL model was loaded succeffully !\n"
          ]
        }
      ],
      "source": [
        "#LOADING MODEL\n",
        "model=\"Bilstm\"\n",
        "\n",
        "if model==\"Bert\":\n",
        "    model_name=\"structured-prediction-srl-bert.2020.12.15.tar.gz\"\n",
        "    path_model=\"models/\"+model_name\n",
        "elif model==\"Bilstm\":\n",
        "    model_name=\"openie-model.2020.03.26.tar.gz\"\n",
        "    path_model=\"models/\"+model_name\n",
        "else:\n",
        "    print(\"Model not found!\")\n",
        "    exit()\n",
        "\n",
        "if os.path.exists(path_model):\n",
        "    print(\"Model found!\")\n",
        "    predictor = Predictor.from_path(path_model)\n",
        "else:\n",
        "    predictor = Predictor.from_path(\"https://storage.googleapis.com/\"+model_name)\n",
        "\n",
        "#TESTING IF THE MODELS ARE LOADED CORRECTLY\n",
        "\n",
        "pred=predictor.predict(\"SRL model was loaded succeffully!\")\n",
        "if pred :\n",
        "    print((\" \").join(pred['words'])) \n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bqIKaBdxF1NF"
      },
      "source": [
        "# Useful functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def append_to_results(results_path,your_result):\n",
        "    \"\"\"Add results to the score board. \n",
        "    your_Results neets to be a dictionary alraedy'\n",
        "    \"\"\"\n",
        "    if type(your_result) != dict:\n",
        "        print(\"your_result needs to be a dictionary!\")\n",
        "        return\n",
        "    \n",
        "    with open(results_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    data.update(your_result)\n",
        "\n",
        "    with open(results_path, 'w') as f:\n",
        "        json.dump(data, f,indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pWslkmPkPFcy"
      },
      "source": [
        "# PREDICATE IDENTIFICATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "#In this folder we have the data for the predicate classidication task. Change the path accordingly if files were moved.\n",
        "path=\"./Data/Predicate_identification/\"\n",
        "results_path=f\"./results_{model}_Predicate_Identification.json\"\n",
        "## we initialize also the dictionary that will contain the results\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump({\"test\":\"failure_rate\"},f,indent=4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PkrGLpJMKSwn"
      },
      "source": [
        "## VOCABULARY+POS"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cCOznqQ_N9-z"
      },
      "source": [
        "### Test for contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failure rate: 0.0. Total number of tests: 18\n"
          ]
        }
      ],
      "source": [
        "data=json.load(open(path+'contracted_predicates.json'))\n",
        "failure_rate=evaluate_PI_contractions_INV(predictor,data)\n",
        "print(f\"Failure rate: {failure_rate}. Total number of tests: {len(data)}\")\n",
        "append_to_results(results_path,{\"contractions\":failure_rate})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ldzmhpP_ODor"
      },
      "source": [
        "### Test for irregular inflections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed for: Gary vext an innocent soul. did not detect vext\n",
            "\n",
            "Failure rate: 2.857142857142857. Total number of tests: 35\n"
          ]
        }
      ],
      "source": [
        "data=json.load(open(path+\"inflected_predicates.json\"))\n",
        "failure_rate=evaluate_PI_inflections_MFT(predictor,data)\n",
        "print(f\"Failure rate: {failure_rate}. Total number of tests: {len(data)}\")\n",
        "append_to_results(results_path,{\"inflected_sentences\":failure_rate})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ROBUTSTNESS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed for: They aet a cool lot. did not detect aet\n",
            "\n",
            "Failure rate: 1.8867924528301887. Total number of tests: 53\n"
          ]
        }
      ],
      "source": [
        "data=json.load(open(path+\"verb_typos_sentence.json\"))\n",
        "failure_rate=evaluate_PI_inflections_MFT(predictor,data)\n",
        "print(f\"Failure rate: {failure_rate}. Total number of tests: {len(data)}\")\n",
        "append_to_results(results_path,{\"inflected_sentences\":failure_rate})\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GIkns01FKdQd"
      },
      "source": [
        "## AMBIGUITY"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "McQzhKS4LCVk"
      },
      "source": [
        "### Experiment with polysemic verbs [POLISEMIC]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37j2bwA3ipm6",
        "outputId": "f14e3a19-db36-42df-c068-3f451f7bd648"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed for: I always turn left at the stop sign. ... [left] found as a verb \n",
            "Failure rate: 3.4482758620689653. Total number of tests: 29\n"
          ]
        }
      ],
      "source": [
        "data=json.load(open(path+'polysem_verbs_sentences.json'))\n",
        "failure_rate=evaluate_PI_Polysem_DIR(predictor,data)\n",
        "print(f\"Failure rate: {failure_rate}. Total number of tests: {len(data)}\")\n",
        "append_to_results(results_path,{\"polysemic_verbs\":failure_rate})\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dY-h4bawK2Ky"
      },
      "source": [
        "### Experiment with verbs being in different roles -ing [GERUNDS]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[paitings] not detected from 'From paintings of his done during that early period', only ['done'] were found\n",
            "[writing] not detected from 'The writing of the thesis took a whole year.', only ['took'] were found\n",
            "[singning] not detected from 'The singing of the birds was a welcome sound in the morning.', only ['was'] were found\n",
            "[reading] not detected from 'What is your reading on the situation?', only ['is'] were found\n",
            "[cleaning] not detected from 'The cleaning of the house was a tedious chore.', only ['was'] were found\n",
            "[washing] not detected from 'The washing of the dishes was a never-ending job.', only ['was', 'ending'] were found\n",
            "[cooking] not detected from 'I have always enjoyed cooking, it is one of my favourite hobbies.', only ['have', 'enjoyed', 'is'] were found\n",
            "[building] not detected from 'The building of the bridge was a remarkable feat of engineering.', only ['was'] were found\n",
            "[cutting] not detected from 'The cutting of the cake was the highlight of the party.', only ['was'] were found\n",
            "[killing] not detected from 'The protests that followed the killing of the priest were widespread.', only ['followed', 'were'] were found\n",
            "[interesting] not detected from 'The movie was interesting to watch', only ['was', 'watch'] were found\n",
            "\n",
            "Failure rate: 73.33333333333333. Total number of tests: 15\n"
          ]
        }
      ],
      "source": [
        "data=json.load(open(path+\"gerunds.json\"))\n",
        "failure_rate=find_roleset_MFT(data,predictor,verboose=True)\n",
        "print(f\"\\nFailure rate: {failure_rate}. Total number of tests: {len(data)}\")\n",
        "append_to_results(results_path,{\"gerunds\":failure_rate})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Su6lKFxWKnbO"
      },
      "source": [
        "## RARITY"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SLANG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed for: I'm gonna meet my friends at the mall. did not detect gonna\n",
            "\n",
            "Failed for: I gotta finish my homework before I can go out. did not detect gotta\n",
            "\n",
            "Failed for: Gimme a slice of pizza, please. did not detect gimme\n",
            "\n",
            "Failed for: Lemme know if you need any help. did not detect lemme\n",
            "\n",
            "Failed for: I'm tryna get in shape for summer. did not detect tryna\n",
            "\n",
            "Failed for: Ima buy a new car next month. did not detect Ima\n",
            "\n",
            "Failed for: I Needa take a break from work. did not detect Needa\n",
            "\n",
            "Failed for: I hafta leave early today for a doctor's appointment. did not detect Hafta\n",
            "\n",
            "Failed for: Whatcha doing this weekend? did not detect Whatcha\n",
            "\n",
            "Failed for: C'mon, let's go to the park. did not detect C'mon\n",
            "\n",
            "Failure rate: 83.33333333333334. Total number of tests: 12\n"
          ]
        }
      ],
      "source": [
        "data=json.load(open(path+\"verbs_slang.json\"))\n",
        "failure_rate=evaluate_PI_inflections_MFT(predictor,data)\n",
        "print(f\"Failure rate: {failure_rate}. Total number of tests: {len(data)}\")\n",
        "append_to_results(results_path,{\"slang_verbs\":failure_rate})\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NEW WORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed for: I'm binge-watching the new TV series this weekend. did not detect binge-watching\n",
            "\n",
            "Failure rate: 10.0. Total number of tests: 10\n"
          ]
        }
      ],
      "source": [
        "data=json.load(open(path+\"new_verbs.json\"))\n",
        "failure_rate=evaluate_PI_inflections_MFT(predictor,data)\n",
        "print(f\"Failure rate: {failure_rate}. Total number of tests: {len(data)}\")\n",
        "append_to_results(results_path,{\"new_verbs\":failure_rate})\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pit7Ny7mvwL7"
      },
      "source": [
        "# AROUGMENTS CLASSIFICATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "#In this folder we have the data for the predicate classidication task. Change the path accordingly if files were moved.\n",
        "path=\"./Data/Argument_classification/\"\n",
        "results_path=f\"./results_{model}_Argument_Classification.json\"\n",
        "## we initialize also the dictionary that will contain the results\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump({\"test_name\":\"failure_rate\"},f,indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VsPA8Ym1P-Qj"
      },
      "source": [
        "## VOCABULAIRTY+POS"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xura9sS5eOrc"
      },
      "source": [
        "###  Entity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " FAILED FOR Sentence:  George went with Jim to the market\n",
            "Predicted BIO tags:  ['B-ARG1', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Adam went with Donald to the mall\n",
            "Predicted BIO tags:  ['B-ARG1', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Marie went with Francis to the mosque\n",
            "Predicted BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-MNR', 'I-ARGM-MNR', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Nicole went with Eleanor to the bakery\n",
            "Predicted BIO tags:  ['B-ARG1', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Louis went with Sarah to the shower\n",
            "Predicted BIO tags:  ['B-ARG0', 'B-V', 'B-ARG1', 'I-ARG1', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Dan went with Julia to the Oscars\n",
            "Predicted BIO tags:  ['B-ARGM-DIS', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Philip went with Frank to the reunion\n",
            "Predicted BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-MNR', 'I-ARGM-MNR', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  George went with Maria to the park\n",
            "Predicted BIO tags:  ['B-ARG1', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Anne went with Emily to the dance\n",
            "Predicted BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-MNR', 'I-ARGM-MNR', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Katie went with Henry to the school\n",
            "Predicted BIO tags:  ['B-ARG1', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Bobby went with Steve to the doctor\n",
            "Predicted BIO tags:  ['B-ARG1', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Eleanor went with Anne to the shooting\n",
            "Predicted BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-ADV', 'I-ARGM-ADV', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Albert went with Florence to the event\n",
            "Predicted BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-MNR', 'I-ARGM-MNR', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  George went with Nicole to the ballet\n",
            "Predicted BIO tags:  ['B-ARG1', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Harry went with Peter to the Y\n",
            "Predicted BIO tags:  ['B-ARG1', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Emily went with Kim to the clinic\n",
            "Predicted BIO tags:  ['B-ARG1', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Hugh went with Frances to the lake\n",
            "Predicted BIO tags:  ['B-ARG1', 'B-V', 'B-ARGM-MNR', 'I-ARGM-MNR', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Leslie went with Katherine to the doctor\n",
            "Predicted BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-MNR', 'I-ARGM-MNR', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Jill went with Mark to the bakery\n",
            "Predicted BIO tags:  ['B-ARG1', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Edwin went with Ray to the fundraiser\n",
            "Predicted BIO tags:  ['B-ARG1', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Kathy went with George to the inauguration\n",
            "Predicted BIO tags:  ['B-ARGM-MNR', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Ed went with Christine to the club\n",
            "Predicted BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-MNR', 'I-ARGM-MNR', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Anna went with Kathy to the bar\n",
            "Predicted BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-MNR', 'I-ARGM-MNR', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Jack went with Gary to the airport\n",
            "Predicted BIO tags:  ['B-ARG1', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Lauren went with Joe to the restaurant\n",
            "Predicted BIO tags:  ['B-ARG1', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Ken went with Lauren to the Y\n",
            "Predicted BIO tags:  ['B-ARG1', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Catherine went with Elizabeth to the prom\n",
            "Predicted BIO tags:  ['B-ARG1', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Don went with William to the church\n",
            "Predicted BIO tags:  ['B-ARG1', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Joan went with Sarah to the conference\n",
            "Predicted BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-MNR', 'I-ARGM-MNR', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Billy went with Emily to the prom\n",
            "Predicted BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-MNR', 'I-ARGM-MNR', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Chris went with Frank to the museum\n",
            "Predicted BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-MNR', 'I-ARGM-MNR', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Brian went with Bill to the inauguration\n",
            "Predicted BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-MNR', 'I-ARGM-MNR', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Mark went with Martha to the house\n",
            "Predicted BIO tags:  ['B-ARG1', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Steven went with Sarah to the mosque\n",
            "Predicted BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-MNR', 'I-ARGM-MNR', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  Amy went with Catherine to the circus\n",
            "Predicted BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-MNR', 'I-ARGM-MNR', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            "Rate of failure:  35.0 Total number of example:  100 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open(path+f\"FirstNames_sents.json\", 'r') as f:\n",
        "        sentences = json.load(f)\n",
        "\n",
        "labels=sentences[\"labels\"]\n",
        "sentences=sentences['data']\n",
        "failure=eval_full_sent_BIOtags(sentences,labels,predictor,verbose=True)\n",
        "\n",
        "print(f\"\\nRate of failure: \",failure,\"Total number of example: \",len(sentences),\"\\n\")\n",
        "\n",
        "append_to_results(results_path,{f\"FistNames\":failure})\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pronouns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " FAILED FOR Sentence:  We went with them to the Capitol\n",
            "Predicted BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-MNR', 'I-ARGM-MNR', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  We went with them to the war\n",
            "Predicted BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-MNR', 'I-ARGM-MNR', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            " FAILED FOR Sentence:  She went with them to the movie\n",
            "Predicted BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-MNR', 'I-ARGM-MNR', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "True BIO tags:  ['B-ARG0', 'B-V', 'B-ARGM-COM', 'I-ARGM-COM', 'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "\n",
            "Rate of failure:  3.0 Total number of example:  100 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open(path+f\"Pronouns_sents.json\", 'r') as f:\n",
        "        sentences = json.load(f)\n",
        "\n",
        "labels=sentences[\"labels\"]\n",
        "sentences=sentences['data']\n",
        "failure=eval_full_sent_BIOtags(sentences,labels,predictor,verbose=True)\n",
        "\n",
        "print(f\"\\nRate of failure: \",failure,\"Total number of example: \",len(sentences),\"\\n\")\n",
        "\n",
        "append_to_results(results_path,{f\"Pronouns\":failure})\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AMBIGUITY/TAXONOMY (PP-ATTACHMENT AMBIGUITY)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PP-ATTACHMENT AMBIGUITY INV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(path+f\"Inv_PPattachments.json\", 'r') as f:\n",
        "        sentences = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input sentences: I went to the resturant by the Hutson\n",
            "Predicted labels for PP: ['I-ARG4', 'B-ARGM-MNR', 'I-ARGM-MNR'] but should have been ['I-ARG4', 'I-ARG4', 'I-ARG4']\n",
            "Input sentences: I fixed the car with a red logo\n",
            "Predicted labels for PP: ['B-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2'] but should have been ['I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1']\n",
            "Input sentences:  I bought a computer with bitcoins\n",
            "Predicted labels for PP: ['I-ARG1', 'I-ARG1'] but should have been ['B-ARGM-MNR', 'I-ARGM-MNR']\n",
            "Input sentences: Lukas helps a man with a disability\n",
            "Predicted labels for PP: ['B-ARG1', 'I-ARG1', 'I-ARG1'] but should have been ['I-ARG2', 'I-ARG2', 'I-ARG2']\n",
            "Input sentences: I drink whiskey with soda\n",
            "Predicted labels for PP: ['B-ARGM-MNR', 'I-ARGM-MNR'] but should have been ['I-ARG1', 'I-ARG1']\n",
            "Input sentences: They Buy a house in cash\n",
            "Predicted labels for PP: ['I-ARG1', 'I-ARG1'] but should have been ['B-ARGM-MNR', 'I-ARGM-MNR']\n",
            "\n",
            "Rate of failure:  100.0 Total number of example:  6 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "rate=eval_PP_INV(sentences,predictor,verbose=True)\n",
        "print(f\"\\nRate of failure: \",rate,\"Total number of example: \",len(sentences),\"\\n\")\n",
        "append_to_results(results_path,{f\"PP_INV\":rate})\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Big PP test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Rate of failure:  19.256550883607556 Total number of example:  1641 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open(path+\"PP_proceesed_test.json\",\"r\") as f:\n",
        "    di=json.load(f)\n",
        "\n",
        "\n",
        "rate,total=eval_PP_MFT(di,predictor,verbose=False)\n",
        "print(f\"\\nRate of failure: \",rate,\"Total number of example: \",total,\"\\n\")\n",
        "append_to_results(results_path,{f\"PP_MFT\":rate})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SPAN IDENTIFICATION"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LONG NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Failure rate: 57.99999999999999. Total number of tests: 100\n"
          ]
        }
      ],
      "source": [
        "\n",
        "with open(path+\"NER_sentences.json\",\"r\") as f:\n",
        "    di=json.load(f)\n",
        "\n",
        "golden=di['labels']\n",
        "sentences=di['data']\n",
        "\n",
        "rate=eval_full_sent_BIOtags(sentences,golden,predictor,verbose=False)\n",
        "print(f\"\\nFailure rate: {rate}. Total number of tests: {len(sentences)}\")\n",
        "append_to_results(results_path,{f\"NER_sents\":rate})\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LONG SPAN ADJECTIVES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'verb': 'is', 'description': 'The Taoist Hungarian asexual friend of mine [V: is] dying too', 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-V', 'O', 'O']}\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "{'verb': 'dying', 'description': '[ARG1: The Taoist Hungarian asexual friend of mine] is [V: dying] [ARGM-ADV: too]', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'B-V', 'B-ARGM-ADV']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': '[ARG1: The Christian Kenyan non - binary friend of mine] [V: is] [ARG2: dead :(]', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG2', 'I-ARG2']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': '[ARG1: The Hindu Kittitian or Nevisian bisexual friend of mine] [V: is] [ARG2: … more]', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG2', 'I-ARG2']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': \"[ARG1: The Shi'a Israeli non - binary friend of mine] [V: is] [ARGM-TMP: now] [ARG2: dead]\", 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARGM-TMP', 'B-ARG2']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': \"[ARG1: The Shi'a Belgian straight friend of mine] [V: is] [ARG2: alive] [ARGM-TMP: today]\", 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG2', 'B-ARGM-TMP']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': '[ARG1: The Muslim Manx gay friend of mine] [V: is] [ARG2: out] [ARGM-TMP: now]', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG2', 'B-ARGM-TMP']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': '[ARG1: The Protestant Yemeni heterosexual friend of mine] [V: is] [ARG2: … –]', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG2', 'I-ARG2']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': 'The Christian Bhutanese homosexual friend of mine [V: is] well loved', 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-V', 'O', 'O']}\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "{'verb': 'loved', 'description': '[ARG1: The Christian Bhutanese homosexual friend of mine] is [ARGM-ADV: well] [V: loved]', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'B-ARGM-ADV', 'B-V']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': 'The Taoist Equatorial Guinean bisexual friend of mine [V: is] dying now', 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-V', 'O', 'O']}\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "{'verb': 'dying', 'description': '[ARG1: The Taoist Equatorial Guinean bisexual friend of mine] is [V: dying] [ARGM-TMP: now]', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'B-V', 'B-ARGM-TMP']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': '[ARG1: The Sunni Grenadian heterosexual friend of mine] [V: is] [ARG2: married] .', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG2', 'O']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': '[ARG1: The Hindu Cambodian non - binary friend of mine] [V: is] [ARG2: married to]', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG2', 'I-ARG2']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': '[ARG1: The Jain Norwegian non - binary friend of mine] [V: is] [ARG2: a Christian]', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG2', 'I-ARG2']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': '[ARG1: The Shinto Burundian trans friend of mine] [V: is] [ARG2: a lesbian]', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG2', 'I-ARG2']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': 'The Ahmadiyya Indonesian heterosexual friend of mine [V: is] dying alone', 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-V', 'O', 'O']}\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "{'verb': 'dying', 'description': '[ARG1: The Ahmadiyya Indonesian heterosexual friend of mine] is [V: dying] [ARGM-MNR: alone]', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'B-V', 'B-ARGM-MNR']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': '[ARG1: The Sunni Uruguayan transgender friend of mine] [V: is] [ARGM-NEG: not] [ARG2: one]', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARGM-NEG', 'B-ARG2']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': 'The Muslim Saint - Martinoise transsexual friend of mine [V: is] dying …', 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-V', 'O', 'O']}\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "{'verb': 'dying', 'description': '[ARG1: The Muslim Saint - Martinoise transsexual friend of mine] is [V: dying] …', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'B-V', 'O']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': '[ARG1: The Eastern Orthodox Nigerian asexual friend of mine] [V: is] [ARGM-NEG: not] [ARG2: Muslim]', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARGM-NEG', 'B-ARG2']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': '[ARG1: The Orthodox Jew Nigerien straight friend of mine] [V: is] [ARG2: dead] [ARGM-TMP: forever]', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG2', 'B-ARGM-TMP']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': '[ARG1: The Agnostic Curaçaoan heterosexual friend of mine] [V: is] [ARG2: dead] [ARGM-TMP: today]', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG2', 'B-ARGM-TMP']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': '[ARG1: The Taoist American Samoan transsexual friend of mine] [V: is] [ARG2: gay] .', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG2', 'O']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': 'The Eastern Orthodox American transsexual friend of mine [V: is] well loved', 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-V', 'O', 'O']}\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "{'verb': 'loved', 'description': '[ARG1: The Eastern Orthodox American transsexual friend of mine] is [ARGM-ADV: well] [V: loved]', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'B-ARGM-ADV', 'B-V']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': '[ARG1: The Anglican Cuban heterosexual friend of mine] [V: is] [ARG2: dead [ …] ]', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG2', 'I-ARG2', 'I-ARG2', 'O']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': '[ARG1: The Taoist Trinidadian or Tobagonian trans friend of mine] [V: is] [ARG2: married …]', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG2', 'I-ARG2']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'trans', 'description': '[ARG0: The Buddhist Liechtenstein] [V: trans] [ARG1: friend] [ARG2: of mine] is here :', 'tags': ['B-ARG0', 'I-ARG0', 'I-ARG0', 'B-V', 'B-ARG1', 'B-ARG2', 'I-ARG2', 'O', 'O', 'O']}\n",
            "['ARG0', 'ARG0', 'ARG0', 'V', 'ARG1', 'ARG2', 'ARG2']\n",
            "{'verb': 'is', 'description': '[ARG1: The Buddhist Liechtenstein trans friend of mine] [V: is] [ARG2: here] :', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG2', 'O']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': 'The Zoroastrian Bangladeshi asexual friend of mine [V: is] [ARG2: well] loved', 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-V', 'B-ARG2', 'O']}\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "{'verb': 'loved', 'description': '[ARG1: The Zoroastrian Bangladeshi asexual friend of mine] is [ARGM-ADV: well] [V: loved]', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'B-ARGM-ADV', 'B-V']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': '[ARG1: The Atheist Burundian transsexual friend of mine] [V: is] [ARG2: gay] .', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG2', 'O']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': \"[ARG1: The Baha'i Faroese queer friend of mine] [V: is] [ARG2: here …]\", 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG2', 'I-ARG2']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': '[ARG1: The Hindu Bolivian bisexual friend of mine] [V: is] [ARG2: dead] :', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG2', 'O']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': '[ARG1: The Jain Cuban transgender friend of mine] [V: is] [ARG2: right] ...', 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG2', 'O']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "{'verb': 'is', 'description': \"[ARG1: The Shi'a Ghanaian pansexual friend of mine] [V: is] [ARG2: dead] .\", 'tags': ['B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG2', 'O']}\n",
            "['ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1']\n",
            "\n",
            "Failure rate: 0.0. Total number of tests: 30\n"
          ]
        }
      ],
      "source": [
        "with open(path+\"longspan_sents.json\",\"r\") as f:\n",
        "    di=json.load(f)\n",
        "sents=di['data']\n",
        "sents=sents[:30]\n",
        "start,end=di[\"indexes\"]\n",
        "rate=eval_spanDetection(sents,start,end,predictor,verbose=False)\n",
        "print(f\"\\nFailure rate: {rate}. Total number of tests: {len(sents)}\")\n",
        "append_to_results(results_path,{f\"longspan\":rate})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "djlQjIyAQIjV"
      },
      "source": [
        "## ROBUSTNESS"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Typos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rate of failure with 1 typos per sentence:  16.0\n",
            "Rate of failure with 2 typos per sentence:  38.0\n",
            "Rate of failure with 3 typos per sentence:  47.0\n",
            "Rate of failure with 4 typos per sentence:  52.0\n"
          ]
        }
      ],
      "source": [
        "for i in range(1,5):#becuase we have 4 files like that\n",
        "\n",
        "    with open(path+f\"sents_{i}_typos.json\", 'r') as f:\n",
        "        sentences = json.load(f)\n",
        "\n",
        "    labels=sentences.pop(\"labels\")\n",
        "    sentences=list(sentences.values())\n",
        "    rate_typos_n=eval_full_sent_BIOtags(sentences,labels,predictor,verbose=False)\n",
        "\n",
        "    print(f\"Rate of failure with {i} typos per sentence: \",rate_typos_n)\n",
        "\n",
        "    append_to_results(results_path,{f\"sents_{i}_typos\":rate_typos_n})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PARAPHRASING"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hGIRP7dVbw_W"
      },
      "source": [
        "### passive and active trasnformarion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error\n",
            "[ARG0: The waiter] [V: served] [ARG1: the meal] . != ['B-ARG0', 'I-ARG0', 'B-V', 'B-ARG1', 'I-ARG1', 'O']\n",
            "[ARG2: The meal] was [V: served] [ARG0: by the waiter] . != ['B-ARG1', 'I-ARG1', 'O', 'B-V', 'B-ARG0', 'I-ARG0', 'I-ARG0', 'O']\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Rate of failure:  5.0 % Total number of example:  20 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open(path+\"activepassive_sentences.json\", 'r') as f:\n",
        "    sentences = json.load(f)\n",
        "\n",
        "\n",
        "labelsActive=sentences.pop(\"labelsActive\")\n",
        "labelsPassive=sentences.pop(\"labelsPassive\")\n",
        "\n",
        "rate=eval_full_sent_BIOtags_INV(sentences,labelsActive,labelsPassive,predictor,verbose=False)\n",
        "\n",
        "print(f\"\\nRate of failure: \",rate,\"% Total number of example: \",len(sentences),\"\\n\")\n",
        "\n",
        "append_to_results(results_path,{f\"ActivePassive\":rate})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7i47oLC8qoSV"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "18hqUeT2Yije"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM6/gtRwphs5tZvfVtQ94Aw",
      "collapsed_sections": [
        "GIkns01FKdQd",
        "McQzhKS4LCVk",
        "dY-h4bawK2Ky",
        "2ybvS9k6Bsin",
        "XbQEiiaFLHtT",
        "xura9sS5eOrc",
        "djlQjIyAQIjV",
        "vFvgduTVSxBn",
        "XzSt0hAmQcYJ",
        "x2IbgB-XRuWQ",
        "qywAgrXmqR8S",
        "LzgT37FWxTPY",
        "JoQIAH4WY2Kl",
        "bZmRzaqwSkTM"
      ],
      "include_colab_link": true,
      "mount_file_id": "1j1qfHJIwReXyO57Aq4uu12y1pKIrYqHv",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "nlptasks",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
