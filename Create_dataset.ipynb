{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ35qPko7VZ7"
      },
      "source": [
        "## This notebook containes all the functions and manually created sentences to further generate (Automatically or semi-automatically) the test sets used in *Challenging-SRL*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjmZ36D27VaA"
      },
      "outputs": [],
      "source": [
        "\"\"\"pip install checklist\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lTp0y5ef7VZ-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "from checklist.editor import Editor\n",
        "from checklist.perturb import Perturb"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YhnXhIPo7VaC"
      },
      "source": [
        "# Predicate identification test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "jQzOoxEC7VaC"
      },
      "outputs": [],
      "source": [
        "#All the sentences generated will be stred in this folder: Might need to create it first\n",
        "path=\"./Data/Predicate_identification/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5nYfnyP7VaD"
      },
      "source": [
        "### Predicate contraction\n",
        "\n",
        "Creation semi manual of the contraction data test.  \n",
        "Some sentences with predicates that can be subjected to this phenomena are written by the author and Checklist will contract or expand the sentence.  \n",
        "The list is not exhaustive and can be always be expanded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QKkINuvI7VaE"
      },
      "outputs": [],
      "source": [
        "def create_contractions(data):\n",
        "    \"\"\"This function creates contractions from a list of sentences.\n",
        "    Either contrated or expanded. Using Checklist Perturbation.contractions\n",
        "\n",
        "    :data is a list of sententes\n",
        "\n",
        "    Returns a list of nested list [sent,sent]\n",
        "    \"\"\"\n",
        "\n",
        "    ret = Perturb.perturb(data, Perturb.contractions)\n",
        "\n",
        "\n",
        "    return ret.data\n",
        "\n",
        "def create_dictionary_Format(data,sents):\n",
        "    \"\"\" \n",
        "    This function is exclusively for formatting a new dictionary.\n",
        "    \n",
        "    :data is {sent:verb_indx}\n",
        "    :sents is list of coupled lists [contracted , expanded]\n",
        "\n",
        "    Output: A nested dictionary\n",
        "    {sentence:(contracted_sentence:verb_indx)}\n",
        "    \"\"\"\n",
        "    co=np.array(sents).T[1]\n",
        "    new_dict = {s: x for s,x in zip(co,data.items())}\n",
        "    return new_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LL-BBU1m7VaF"
      },
      "outputs": [],
      "source": [
        "#THIS sentences were manually generated taking inspiration from Checklist perturbator.py where a various contractions are listed\n",
        "\n",
        "data = {'it\\'s a wonderfull day': 1,\n",
        "        \"where did he go?\":1,\n",
        "        \"There's some pesto left\": 1,\n",
        "        \"He was ought not to do it\": 2,\n",
        "        \"I could've tried that as well\": 2,\n",
        "        \"They will leave the house\": 1,\n",
        "        \"That would be creazy\": 1,\n",
        "        \"we are here\": 1,\n",
        "        \"Mark had not see that coming\": 1,\n",
        "        \"She will be a great candidate\": 1,\n",
        "        \"Mary is not a nurse.\": 1,\n",
        "        \"He's gone already\": 1,\n",
        "        \"I would like some tea\": 1,\n",
        "        \"who is there?\": 1,\n",
        "        \"I could not eat some food now\": 1,\n",
        "        \"We've decided to change house\": 1,\n",
        "        \"I must not lose my temper\": 1,\n",
        "        \"You might not want to do that\": 1\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QqemIl1x7VaH",
        "outputId": "0f9d58eb-e0d0-4b24-c6dc-3011c3082187"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "({'it is a wonderfull day': (\"it's a wonderfull day\", 1),\n",
              "  \"where'd he go?\": ('where did he go?', 1),\n",
              "  'There is some pesto left': (\"There's some pesto left\", 1),\n",
              "  \"He was oughtn't to do it\": ('He was ought not to do it', 2),\n",
              "  'I could have tried that as well': (\"I could've tried that as well\", 2),\n",
              "  \"They'll leave the house\": ('They will leave the house', 1),\n",
              "  \"That'd be creazy\": ('That would be creazy', 1),\n",
              "  \"we're here\": ('we are here', 1),\n",
              "  \"Mark hadn't see that coming\": ('Mark had not see that coming', 1),\n",
              "  \"She'll be a great candidate\": ('She will be a great candidate', 1),\n",
              "  \"Mary isn't a nurse.\": ('Mary is not a nurse.', 1),\n",
              "  'He is gone already': (\"He's gone already\", 1),\n",
              "  \"I'd like some tea\": ('I would like some tea', 1),\n",
              "  \"who's there?\": ('who is there?', 1),\n",
              "  \"I couldn't eat some food now\": ('I could not eat some food now', 1),\n",
              "  'We have decided to change house': (\"We've decided to change house\", 1),\n",
              "  \"I mustn't lose my temper\": ('I must not lose my temper', 1),\n",
              "  \"You mightn't want to do that\": ('You might not want to do that', 1)},\n",
              " 18)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sents=create_contractions(list(data.keys()))\n",
        "out_dict=create_dictionary_Format(data,sents)\n",
        "out_dict,len(out_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wTi7AYs67VaI"
      },
      "outputs": [],
      "source": [
        "with open(path+\"contracted_sentences.json\",\"w\") as f:\n",
        "    json.dump(out_dict,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9wUvZw_7VaI"
      },
      "source": [
        "### Predicate irregular inflections\n",
        "Semi automatic creation of sentences using a list of irregular inflected verb.  \n",
        "List was found online / written by the authros. Example sentences are contructed with Large Language Model RoBerta integrated in CheckList library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "y5jEOMl97VaJ"
      },
      "outputs": [],
      "source": [
        "def create_inflected_sentences(irregular_inflections):\n",
        "    \"\"\"This function creates a dictionary in the shape {label:sentence}. Where label is the inflected verb form.\n",
        "    :irregular_inflections is a list of the irregular verbs to be used.\n",
        "    returns Dict\n",
        "    \"\"\"\n",
        "    editor = Editor()\n",
        "    #This will add the irregular verbs to the lexicon so that we can use them in the template.\n",
        "    editor.add_lexicon('irr_verb', irregular_inflections,remove_duplicates=True)\n",
        "\n",
        "    #This will create 1000 samples sentences and their lables will be the irregular verb picked.\n",
        "    #{mask} is a special token that will be replaced by a random word suggested by the Language model.\n",
        "    #{fist_name} is a special token that will be replaced by a random first name in the lexicon.\n",
        "    ret = editor.template('{first_name} {irr_verb} {a:mask} {mask}.',nsamples=100,labels='{irr_verb}')\n",
        "\n",
        "    #This creates a dictionary in the shape {label:sentence}. Where label is the verb.\n",
        "    #Carefull, this will be much smaller than the number of samples because some of the sentences will be duplicates.\n",
        "    inflected_sentences=dict(zip(ret.labels,ret.data))\n",
        "\n",
        "\n",
        "    return inflected_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "sXQMkv8I7VaK"
      },
      "outputs": [],
      "source": [
        "#MIght take a while cuz RoberTa is a big model\n",
        "irregular_inflections=['Beheld', 'Dwelt', 'Flung', 'Broadcast', 'Clung', 'Dared', 'Fitted', 'Forgave', 'Grinded', 'Hanged', 'Knelt', 'Laid', 'Led', 'Leant', 'Molten', 'Mistook', 'Proved', 'Rose', 'Sawn', 'Sought', 'Sewed', 'Shaven', 'Slit', 'Snuck', 'Span', 'Spoiled', 'Spring', 'Stuck', 'Strode', 'Struck', 'Swung', 'Torn', 'Undertook', 'Vext', 'Wet', 'Wrote']\n",
        "irregular_inflections=[x.lower() for x in irregular_inflections]\n",
        "inflected_sentences=create_inflected_sentences(irregular_inflections)\n",
        "len(inflected_sentences),inflected_sentences\n",
        "with open(path+\"inflected_sentences.json\",\"w\") as f:\n",
        "    json.dump(inflected_sentences,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ozk3dcz7VaN"
      },
      "source": [
        "### Typos\n",
        "For the creation of the this dataset we take a list of verbs (possibly transitive) from ChatGPT/the internet. For each verb we add a typo by switching two characters with the help of Checklist perturbator class. We then use Checklist template to fill a tamplate with the perturbated verb."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "U3pQ8jrg89_G"
      },
      "outputs": [],
      "source": [
        "def create_verb_typos(verb_list):\n",
        "  \"\"\"This function creates a list of sentences with a perturbate verbs (typos).\n",
        "  It first generated  the list of wrong verbs from the input list and then create as many sentences.\n",
        "  \"\"\"\n",
        "  editor=Editor()\n",
        "  verb_typos=[Perturb.add_typos(x) for x in verb_list]\n",
        "\n",
        "  editor.add_lexicon('verb_typos', verb_typos,remove_duplicates=True)\n",
        "  editor.add_lexicon('adj', ['good', 'bad', 'great', 'terrible','wierd','cool','aweful'])\n",
        "\n",
        "  ret = editor.template('They {verb_typos} a {adj} {mask}.',nsamples=len(verb_list), remove_duplicates=True,labels='{verb_typos}')\n",
        "  return dict(zip(ret.labels, ret.data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-fdhLsGTMgtf"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m verb_list\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mBeheld\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFlung\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mBroadcast\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mForgave\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mGrinded\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mHanged\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLaid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLed\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLeant\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMolten\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMistook\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mProved\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSawn\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSought\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSewed\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mShaven\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSlit\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSnuck\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSpan\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSpoiled\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mStuck\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mStrode\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mStruck\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSwung\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTorn\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mUndertook\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mVext\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mWet\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mWrote\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39meat\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdrink\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mthrow\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcatch\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwrite\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mread\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhit\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mkick\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mopen\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mclose\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcook\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbake\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpaint\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdraw\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbuild\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbreak\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrepair\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mclean\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwash\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdrive\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mride\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcarry\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlift\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mplay\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msing\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdance\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlove\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mneed\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwant\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlike\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdislike\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mteach\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlearn\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39munderstand\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mknow\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mremember\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mforget\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhelp\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhurt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mshow\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m sents\u001b[39m=\u001b[39mcreate_verb_typos(verb_list)\n\u001b[1;32m      5\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mverb_typos_sentence.json\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      6\u001b[0m     json\u001b[39m.\u001b[39mdump(sents,f)\n",
            "Cell \u001b[0;32mIn[30], line 11\u001b[0m, in \u001b[0;36mcreate_verb_typos\u001b[0;34m(verb_list)\u001b[0m\n\u001b[1;32m      8\u001b[0m editor\u001b[39m.\u001b[39madd_lexicon(\u001b[39m'\u001b[39m\u001b[39mverb_typos\u001b[39m\u001b[39m'\u001b[39m, verb_typos,remove_duplicates\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m editor\u001b[39m.\u001b[39madd_lexicon(\u001b[39m'\u001b[39m\u001b[39madj\u001b[39m\u001b[39m'\u001b[39m, [\u001b[39m'\u001b[39m\u001b[39mgood\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbad\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgreat\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mterrible\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mwierd\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcool\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39maweful\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 11\u001b[0m ret \u001b[39m=\u001b[39m editor\u001b[39m.\u001b[39;49mtemplate(\u001b[39m'\u001b[39;49m\u001b[39mThey \u001b[39;49m\u001b[39m{verb_typos}\u001b[39;49;00m\u001b[39m a \u001b[39;49m\u001b[39m{adj}\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m{mask}\u001b[39;49;00m\u001b[39m.\u001b[39;49m\u001b[39m'\u001b[39;49m,nsamples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(verb_list), remove_duplicates\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,labels\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{verb_typos}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(ret\u001b[39m.\u001b[39mlabels, ret\u001b[39m.\u001b[39mdata))\n",
            "File \u001b[0;32m~/miniconda3/envs/nlptasks/lib/python3.10/site-packages/checklist/editor.py:672\u001b[0m, in \u001b[0;36mEditor.template\u001b[0;34m(self, templates, nsamples, product, remove_duplicates, mask_only, unroll, labels, meta, save, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m samp \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtemplate(ts, nsamples\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, remove_duplicates\u001b[39m=\u001b[39mremove_duplicates,\n\u001b[1;32m    669\u001b[0m                      thisisaratherlongtokenthatwillnotexist\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39man\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\u001b[39m.\u001b[39mdata\n\u001b[1;32m    670\u001b[0m \u001b[39m# print(samp)\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \u001b[39m# print(len([x for x in samp if ' an ' in x[0]]))\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m samp \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mreplace(tok, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtg\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mmask_token) \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m samp \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m y][:\u001b[39m20\u001b[39m]\n\u001b[1;32m    673\u001b[0m samp \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(samp))\n\u001b[1;32m    674\u001b[0m \u001b[39m# print(samp)\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/nlptasks/lib/python3.10/site-packages/checklist/editor.py:672\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    668\u001b[0m samp \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtemplate(ts, nsamples\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, remove_duplicates\u001b[39m=\u001b[39mremove_duplicates,\n\u001b[1;32m    669\u001b[0m                      thisisaratherlongtokenthatwillnotexist\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39man\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\u001b[39m.\u001b[39mdata\n\u001b[1;32m    670\u001b[0m \u001b[39m# print(samp)\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \u001b[39m# print(len([x for x in samp if ' an ' in x[0]]))\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m samp \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mreplace(tok, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtg\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mmask_token) \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m samp \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m y][:\u001b[39m20\u001b[39m]\n\u001b[1;32m    673\u001b[0m samp \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(samp))\n\u001b[1;32m    674\u001b[0m \u001b[39m# print(samp)\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/nlptasks/lib/python3.10/site-packages/checklist/editor.py:306\u001b[0m, in \u001b[0;36mEditor.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtext_generation\u001b[39;00m \u001b[39mimport\u001b[39;00m TextGenerator\n\u001b[1;32m    305\u001b[0m     params \u001b[39m=\u001b[39m multilingual_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtg_params)\n\u001b[0;32m--> 306\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtg \u001b[39m=\u001b[39m TextGenerator(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    307\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtg\n\u001b[1;32m    308\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/nlptasks/lib/python3.10/site-packages/checklist/text_generation.py:83\u001b[0m, in \u001b[0;36mTextGenerator.__init__\u001b[0;34m(self, url, model_name, prefix_sentence, allow_word_pieces, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39m# self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39m# self.model = BertForMaskedLM.from_pretrained('bert-base-cased')\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m---> 83\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m AutoModelForMaskedLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n\u001b[1;32m     84\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     85\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:446\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    445\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 446\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    447\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    448\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:2007\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2001\u001b[0m     archive_file \u001b[39m=\u001b[39m hf_bucket_url(\n\u001b[1;32m   2002\u001b[0m         pretrained_model_name_or_path, filename\u001b[39m=\u001b[39mfilename, revision\u001b[39m=\u001b[39mrevision, mirror\u001b[39m=\u001b[39mmirror\n\u001b[1;32m   2003\u001b[0m     )\n\u001b[1;32m   2005\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2006\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m-> 2007\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_path(\n\u001b[1;32m   2008\u001b[0m         archive_file,\n\u001b[1;32m   2009\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2010\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   2011\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   2012\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   2013\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   2014\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   2015\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   2016\u001b[0m     )\n\u001b[1;32m   2018\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m   2019\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2020\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2021\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to pass a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2022\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtoken having permission to this repo with `use_auth_token` or log in with `huggingface-cli \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2023\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlogin` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2024\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:284\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    280\u001b[0m     local_files_only \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m    283\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[1;32m    285\u001b[0m         url_or_filename,\n\u001b[1;32m    286\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    287\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    288\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    289\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    290\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    291\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    292\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    293\u001b[0m     )\n\u001b[1;32m    294\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    295\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     output_path \u001b[39m=\u001b[39m url_or_filename\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:494\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m local_files_only:\n\u001b[1;32m    493\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 494\u001b[0m         r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mhead(url, headers\u001b[39m=\u001b[39;49mheaders, allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, proxies\u001b[39m=\u001b[39;49mproxies, timeout\u001b[39m=\u001b[39;49metag_timeout)\n\u001b[1;32m    495\u001b[0m         _raise_for_status(r)\n\u001b[1;32m    496\u001b[0m         etag \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mX-Linked-Etag\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m r\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mETag\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/api.py:100\u001b[0m, in \u001b[0;36mhead\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a HEAD request.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[1;32m     91\u001b[0m \u001b[39m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 100\u001b[0m \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mhead\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[1;32m   1045\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1046\u001b[0m         (\n\u001b[1;32m   1047\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1053\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:419\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    411\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_certs\n\u001b[1;32m    412\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_cert_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(context, \u001b[39m\"\u001b[39m\u001b[39mload_default_certs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    416\u001b[0m ):\n\u001b[1;32m    417\u001b[0m     context\u001b[39m.\u001b[39mload_default_certs()\n\u001b[0;32m--> 419\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    420\u001b[0m     sock\u001b[39m=\u001b[39;49mconn,\n\u001b[1;32m    421\u001b[0m     keyfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[1;32m    422\u001b[0m     certfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[1;32m    423\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[1;32m    424\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[1;32m    425\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[1;32m    426\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[1;32m    427\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    428\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[1;32m    429\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[1;32m    430\u001b[0m )\n\u001b[1;32m    432\u001b[0m \u001b[39m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[39m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[39m# for the host.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    436\u001b[0m     default_ssl_context\n\u001b[1;32m    437\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_version \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock, \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    439\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock\u001b[39m.\u001b[39mversion() \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mTLSv1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTLSv1.1\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m    440\u001b[0m ):\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/util/ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    437\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    438\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn HTTPS request has been made, but the SNI (Server Name \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIndication) extension to TLS is not available on this platform. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m         SNIMissingWarning,\n\u001b[1;32m    446\u001b[0m     )\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m send_sni:\n\u001b[0;32m--> 449\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[1;32m    450\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[39m=\u001b[39;49mserver_hostname\n\u001b[1;32m    451\u001b[0m     )\n\u001b[1;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/util/ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[39mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m    492\u001b[0m \u001b[39mif\u001b[39;00m server_hostname:\n\u001b[0;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n\u001b[1;32m    494\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    495\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39mwrap_socket(sock)\n",
            "File \u001b[0;32m~/miniconda3/envs/nlptasks/lib/python3.10/ssl.py:513\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    508\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    509\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    510\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    511\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    512\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[1;32m    514\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    515\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[1;32m    516\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[1;32m    517\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[1;32m    518\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    519\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    520\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[1;32m    521\u001b[0m     )\n",
            "File \u001b[0;32m~/miniconda3/envs/nlptasks/lib/python3.10/ssl.py:1071\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m   1069\u001b[0m             \u001b[39m# non-blocking\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1071\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1072\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1073\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
            "File \u001b[0;32m~/miniconda3/envs/nlptasks/lib/python3.10/ssl.py:1342\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mand\u001b[39;00m block:\n\u001b[1;32m   1341\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1342\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1343\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(timeout)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "verb_list=['Beheld', 'Flung', 'Broadcast', 'Forgave', 'Grinded', 'Hanged', 'Laid', 'Led', 'Leant', 'Molten', 'Mistook', 'Proved', 'Sawn', 'Sought', 'Sewed', 'Shaven', 'Slit', 'Snuck', 'Span', 'Spoiled', 'Stuck', 'Strode', 'Struck', 'Swung', 'Torn', 'Undertook', 'Vext', 'Wet', 'Wrote','eat', 'drink', 'throw', 'catch', 'write', 'read', 'hit', 'kick', 'open', 'close', 'cook', 'bake', 'paint', 'draw', 'build', 'break', 'repair', 'clean', 'wash', 'drive', 'ride', 'carry', 'lift', 'play', 'sing', 'dance', 'love', 'hate', 'need', 'want', 'like', 'dislike', 'teach', 'learn', 'understand', 'know', 'remember', 'forget', 'help', 'hurt', 'show']\n",
        "\n",
        "sents=create_verb_typos(verb_list)\n",
        "\n",
        "with open(path+\"verb_typos_sentence.json\",\"w\") as f:\n",
        "    json.dump(sents,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FiJlUDfJREE"
      },
      "source": [
        "### Slang\n",
        "This data were created with the help of chat gpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RrcwAUYlJhfu"
      },
      "outputs": [],
      "source": [
        "sentences = {\n",
        "    \"wanna\": \"I wanna go to the movies tonight.\",\n",
        "    \"gonna\": \"I'm gonna meet my friends at the mall.\",\n",
        "    \"gotta\": \"I gotta finish my homework before I can go out.\",\n",
        "    \"gimme\": \"Gimme a slice of pizza, please.\",\n",
        "    \"lemme\": \"Lemme know if you need any help.\",\n",
        "    \"dunno\": \"I dunno what to wear to the party.\",\n",
        "    \"tryna\": \"I'm tryna get in shape for summer.\",\n",
        "    \"Ima\": \"Ima buy a new car next month.\",\n",
        "    \"Needa\": \"I Needa take a break from work.\",\n",
        "    \"Hafta\": \"I hafta leave early today for a doctor's appointment.\",\n",
        "    \"Whatcha\": \"Whatcha doing this weekend?\",\n",
        "    \"C'mon\": \"C'mon, let's go to the park.\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub4e61ymJhh2",
        "outputId": "0c6f231d-75e5-435e-c26a-e0b604a2fd71"
      },
      "outputs": [],
      "source": [
        "with open(path+\"verbs_slang.json\",\"w\") as f:\n",
        "    json.dump(sentences,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIeAxpC_Pc7r"
      },
      "source": [
        "### New verbs\n",
        "This data were created mostly by manually looking for new verbs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gPgzRQVXPWpB"
      },
      "outputs": [],
      "source": [
        "sentences = {\n",
        "\"google\": \"I need to google the address of the restaurant.\",\n",
        "    \"zoom\": \"Let's zoom the meeting instead of meeting in person.\",\n",
        "    \"binge-watching\": \"I'm binge-watching the new TV series this weekend.\",\n",
        "    \"adulting\": \"I don't feel like adulting today, can we just stay in bed?\",\n",
        "    \"ghosted\": \"He ghosted me after our first date and never replied to my messages.\",\n",
        "    \"tweeting\":\"He has been tweeting aweful stuff\",\n",
        "    \"flexed\": \"She flexed her designer bags on social media.\",\n",
        "    \"stan\": \"I stan this new artist, their music is amazing!\",\n",
        "    \"greenwash\":\"They like to greenwash their people\",\n",
        "    \"terraform\":\"If we terraform another plants we have to make sure we build an equal society\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1tM0bd-bRAV4"
      },
      "outputs": [],
      "source": [
        "with open(path+\"new_verbs.json\",\"w\") as f:\n",
        "    json.dump(sentences,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CInsLWc7VaP"
      },
      "source": [
        "# ARGOUMENTS CLASSIFICATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "RscMahMY7VaP"
      },
      "outputs": [],
      "source": [
        "#All the sentences generated will be stred in this folder:\n",
        "path=\"./Data/Argument_classification/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiwvQO-c7VaP"
      },
      "source": [
        "### Typos \n",
        "Here we create perturbed sentences with Checklist.\n",
        "It is possible to perturbate the sentences up to n times.\n",
        "From 1 to n typos dataset will be saved as well (to make a nice comparison)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mk314pDVAll"
      },
      "outputs": [],
      "source": [
        "def add_n_typos(sents,n):\n",
        "  \"\"\"\n",
        "  This functions perturb a sentence with n typos\n",
        "  Returns a dict like {original:Typos_sents}\n",
        "  \"\"\"\n",
        "  original=sents\n",
        "  for x in range(n):\n",
        "    sents=[Perturb.add_typos(x) for x in sents]\n",
        "  return dict(zip(original,sents))\n",
        "  \n",
        "def create_Multiple_typos_sentences(sents,n):\n",
        "  \"\"\"\n",
        "  This function creates n dictionaries with sentences perturbed n times where n goes from 1 to n\n",
        "  reutrns dict of dicts\n",
        "  \"\"\"\n",
        "  dict_typos={}\n",
        "  for i in range(1,n+1):\n",
        "    dict_typos[i]=add_n_typos(sents,i)\n",
        "  return dict_typos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPdDCXg27VaR"
      },
      "outputs": [],
      "source": [
        "editor = Editor()\n",
        "ret = editor.template('{first_name} and {mask} {mask} {a:mask} {mask}.',nsamples=100,keep_original=True)\n",
        "correct_sents=ret.data\n",
        "all_typos_sents=create_Multiple_typos_sentences(correct_sents,4)\n",
        "\n",
        "for i in (all_typos_sents.keys()):\n",
        "  print(i)\n",
        "  with open(path+f\"sents_{i}_typos.json\",\"w\") as f:\n",
        "    json.dump(all_typos_sents[i],f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_NER_sent():\n",
        "    golden_tags=\"['B-ARG0', 'I-ARG0', 'I-ARG0', 'B-V', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'B-ARGM-LOC', 'I-ARGM-LOC']\"\n",
        "    editor = Editor()\n",
        "    first1 = [x.split()[0] for x in editor.lexicons.male_from.Vietnam +  editor.lexicons.female_from.Vietnam]\n",
        "    first2 = [x.split()[0] for x in editor.lexicons.male_from.Indonesia +  editor.lexicons.female_from.Nepal]\n",
        "    last = [x.split()[0] for x in editor.lexicons.last_from.Cameroon + editor.lexicons.last_from.Palau]\n",
        "    cityy=[x for x in editor.lexicons.country_city.Ethiopia + editor.lexicons.country_city.Russia+editor.lexicons.country_city.South_Africa if len(x.split())==1] \n",
        "    t = editor.template(' {first_name} {first_name1} {last_name1} saw {first1_name2} {first1_name3} {last_name4} in {city}', first_name=first1,first1_name=first2, last_name=last, city=cityy,meta=True, nsamples=100)\n",
        "    return {golden_tags:t.data}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "di=create_NER_sent()\n",
        "with open(path+\"NER_sentences.json\",\"w\") as f:\n",
        "    json.dump(di,f)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Coreference Study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Those are the keys of luis'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"It annoys me the results of the election\"\n",
        "\"Those are the keys of luis\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PP attachment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[\"I fixed the car with a red logo\",\"I fixed the car with a wretch\"]\n",
        "[\"I bought a computer with GPU\",\" I bought a computer wit bitcoins\"]\n",
        "[\"I went to the resturant by the Hutsin\", \"I went to the resturant by bike\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlpTaskKernel",
      "language": "python",
      "name": "nlptaskkernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
