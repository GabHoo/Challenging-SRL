{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ35qPko7VZ7"
      },
      "source": [
        "## This notebook containes all the functions and manually created sentences to further generate (Automatically or semi-automatically) the test sets used in *Challenging-SRL*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjmZ36D27VaA"
      },
      "outputs": [],
      "source": [
        "\"\"\"pip install checklist\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lTp0y5ef7VZ-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "from checklist.editor import Editor\n",
        "from checklist.perturb import Perturb"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YhnXhIPo7VaC"
      },
      "source": [
        "# Predicate identification test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jQzOoxEC7VaC"
      },
      "outputs": [],
      "source": [
        "#All the sentences generated will be stred in this folder: Might need to create it first\n",
        "path=\"./Data/Predicate_identification/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5nYfnyP7VaD"
      },
      "source": [
        "### Predicate contraction\n",
        "\n",
        "Creation semi manual of the contraction data test.  \n",
        "Some sentences with predicates that can be subjected to this phenomena are written by the author and Checklist will contract or expand the sentence.  \n",
        "The list is not exhaustive and can be always be expanded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "QKkINuvI7VaE"
      },
      "outputs": [],
      "source": [
        "def create_contractions(data):\n",
        "    \"\"\"This function creates contractions from a list of sentences.\n",
        "    Either contrated or expanded. Using Checklist Perturbation.contractions\n",
        "\n",
        "    :data is a list of sententes\n",
        "\n",
        "    Returns a list of nested list [sent,sent]\n",
        "    \"\"\"\n",
        "\n",
        "    ret = Perturb.perturb(list(data.keys()), Perturb.contractions)\n",
        "    \n",
        "    #re-merge the couple of sentence with the index so to have all in one data str\n",
        "    return [(x,i) for x,i in zip(ret.data,data.values())]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "LL-BBU1m7VaF"
      },
      "outputs": [],
      "source": [
        "#THIS sentences were manually generated taking inspiration from Checklist perturbator.py where a various contractions are listed\n",
        "\n",
        "data = {'it\\'s a wonderfull day': 1,\n",
        "        \"where did he go?\":1,\n",
        "        \"There's some pesto left\": 1,\n",
        "        \"He was ought not to do it\": 2,\n",
        "        \"I could've tried that as well\": 2,\n",
        "        \"They will leave the house\": 1,\n",
        "        \"That would be creazy\": 1,\n",
        "        \"we are here\": 1,\n",
        "        \"Mark had not see that coming\": 1,\n",
        "        \"She will be a great candidate\": 1,\n",
        "        \"Mary is not a nurse.\": 1,\n",
        "        \"He's gone already\": 1,\n",
        "        \"I would like some tea\": 1,\n",
        "        \"who is there?\": 1,\n",
        "        \"I could not eat some food now\": 1,\n",
        "        \"We've decided to change house\": 1,\n",
        "        \"I must not lose my temper\": 1,\n",
        "        \"You might not want to do that\": 1\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "QqemIl1x7VaH",
        "outputId": "0f9d58eb-e0d0-4b24-c6dc-3011c3082187"
      },
      "outputs": [],
      "source": [
        "sents=create_contractions(data)\n",
        "#print(sents)\n",
        "with open(path+\"contracted_predicates.json\",\"w\") as f:\n",
        "    json.dump(sents,f,indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9wUvZw_7VaI"
      },
      "source": [
        "### Predicate irregular inflections\n",
        "Semi automatic creation of sentences using a list of irregular inflected verb.  \n",
        "List was found online / written by the authros. Example sentences are contructed with Large Language Model RoBerta integrated in CheckList library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "y5jEOMl97VaJ"
      },
      "outputs": [],
      "source": [
        "def create_inflected_sentences(irregular_inflections):\n",
        "    \"\"\"This function creates a dictionary in the shape {label:sentence}. Where label is the inflected verb form.\n",
        "    :irregular_inflections is a list of the irregular verbs to be used.\n",
        "    returns Dict\n",
        "    \"\"\"\n",
        "    editor = Editor()\n",
        "    #This will add the irregular verbs to the lexicon so that we can use them in the template.\n",
        "    editor.add_lexicon('irr_verb', irregular_inflections,remove_duplicates=True)\n",
        "\n",
        "    #This will create 1000 samples sentences and their lables will be the irregular verb picked.\n",
        "    #{mask} is a special token that will be replaced by a random word suggested by the Language model.\n",
        "    #{fist_name} is a special token that will be replaced by a random first name in the lexicon.\n",
        "    ret = editor.template('{first_name} {irr_verb} {a:mask} {mask}.',nsamples=100,labels='{irr_verb}')\n",
        "\n",
        "    #This creates a dictionary in the shape {label:sentence}. Where label is the verb.\n",
        "    #Carefull, this will be much smaller than the number of samples because some of the sentences will be duplicates.\n",
        "    inflected_sentences=dict(zip(ret.labels,ret.data))\n",
        "\n",
        "\n",
        "    return inflected_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sXQMkv8I7VaK"
      },
      "outputs": [],
      "source": [
        "#MIght take a while cuz RoberTa is a big model\n",
        "irregular_inflections=['Beheld', 'Dwelt', 'Flung', 'Broadcast', 'Clung', 'Dared', 'Fitted', 'Forgave', 'Grinded', 'Hanged', 'Knelt', 'Laid', 'Led', 'Leant', 'Molten', 'Mistook', 'Proved', 'Rose', 'Sawn', 'Sought', 'Sewed', 'Shaven', 'Slit', 'Snuck', 'Span', 'Spoiled', 'Spring', 'Stuck', 'Strode', 'Struck', 'Swung', 'Torn', 'Undertook', 'Vext', 'Wet', 'Wrote']\n",
        "irregular_inflections=[x.lower() for x in irregular_inflections]\n",
        "inflected_sentences=create_inflected_sentences(irregular_inflections)\n",
        "len(inflected_sentences),inflected_sentences\n",
        "with open(path+\"inflected_predicates.json\",\"w\") as f:\n",
        "    json.dump(inflected_sentences,f,indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ozk3dcz7VaN"
      },
      "source": [
        "### Typos\n",
        "For the creation of the this dataset we take a list of verbs (possibly transitive) from ChatGPT/the internet. For each verb we add a typo by switching two characters with the help of Checklist perturbator class. We then use Checklist template to fill a tamplate with the perturbated verb."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "U3pQ8jrg89_G"
      },
      "outputs": [],
      "source": [
        "def create_verb_typos(verb_list):\n",
        "  \"\"\"This function creates a list of sentences with a perturbate verbs (typos).\n",
        "  It first generated  the list of wrong verbs from the input list and then create as many sentences.\n",
        "  \"\"\"\n",
        "  editor=Editor()\n",
        "  verb_typos=[Perturb.add_typos(x) for x in verb_list]\n",
        "\n",
        "  editor.add_lexicon('verb_typos', verb_typos,remove_duplicates=True)\n",
        "  editor.add_lexicon('adj', ['good', 'bad', 'great', 'terrible','wierd','cool','aweful'])\n",
        "\n",
        "  ret = editor.template('They {verb_typos} a {adj} {mask}.',nsamples=100, remove_duplicates=True,labels='{verb_typos}')\n",
        "  return dict(zip(ret.labels, ret.data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "-fdhLsGTMgtf"
      },
      "outputs": [],
      "source": [
        "verb_list=['Beheld', 'Flung', 'Broadcast', 'Forgave', 'Grinded', 'Hanged', 'Laid', 'Led', 'Leant', 'Molten', 'Mistook', 'Proved', 'Sawn', 'Sought', 'Sewed', 'Shaven', 'Slit', 'Snuck', 'Span', 'Spoiled', 'Stuck', 'Strode', 'Struck', 'Swung', 'Torn', 'Undertook', 'Vext', 'Wet', 'Wrote','eat', 'drink', 'throw', 'catch', 'write', 'read', 'hit', 'kick', 'open', 'close', 'cook', 'bake', 'paint', 'draw', 'build', 'break', 'repair', 'clean', 'wash', 'drive', 'ride', 'carry', 'lift', 'play', 'sing', 'dance', 'love', 'hate', 'need', 'want', 'like', 'dislike', 'teach', 'learn', 'understand', 'know', 'remember', 'forget', 'help', 'hurt', 'show']\n",
        "verb_list=[x.lower() for x in verb_list]\n",
        "sents=create_verb_typos(verb_list)\n",
        "\n",
        "with open(path+\"verb_typos_sentence.json\",\"w\") as f:\n",
        "    json.dump(sents,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FiJlUDfJREE"
      },
      "source": [
        "### Slang\n",
        "This data were created with the help of chat gpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RrcwAUYlJhfu"
      },
      "outputs": [],
      "source": [
        "sentences = {\n",
        "    \"wanna\": \"I wanna go to the movies tonight.\",\n",
        "    \"gonna\": \"I'm gonna meet my friends at the mall.\",\n",
        "    \"gotta\": \"I gotta finish my homework before I can go out.\",\n",
        "    \"gimme\": \"Gimme a slice of pizza, please.\",\n",
        "    \"lemme\": \"Lemme know if you need any help.\",\n",
        "    \"dunno\": \"I dunno what to wear to the party.\",\n",
        "    \"tryna\": \"I'm tryna get in shape for summer.\",\n",
        "    \"Ima\": \"Ima buy a new car next month.\",\n",
        "    \"Needa\": \"I Needa take a break from work.\",\n",
        "    \"Hafta\": \"I hafta leave early today for a doctor's appointment.\",\n",
        "    \"Whatcha\": \"Whatcha doing this weekend?\",\n",
        "    \"C'mon\": \"C'mon, let's go to the park.\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub4e61ymJhh2",
        "outputId": "0c6f231d-75e5-435e-c26a-e0b604a2fd71"
      },
      "outputs": [],
      "source": [
        "with open(path+\"verbs_slang.json\",\"w\") as f:\n",
        "    json.dump(sentences,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIeAxpC_Pc7r"
      },
      "source": [
        "### New verbs\n",
        "This data were created mostly by manually looking for new verbs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gPgzRQVXPWpB"
      },
      "outputs": [],
      "source": [
        "sentences = {\n",
        "\"google\": \"I need to google the address of the restaurant.\",\n",
        "    \"zoom\": \"Let's zoom the meeting instead of meeting in person.\",\n",
        "    \"binge-watching\": \"I'm binge-watching the new TV series this weekend.\",\n",
        "    \"adulting\": \"I don't feel like adulting today, can we just stay in bed?\",\n",
        "    \"ghosted\": \"He ghosted me after our first date and never replied to my messages.\",\n",
        "    \"tweeting\":\"He has been tweeting aweful stuff\",\n",
        "    \"flexed\": \"She flexed her designer bags on social media.\",\n",
        "    \"stan\": \"I stan this new artist, their music is amazing!\",\n",
        "    \"greenwash\":\"They like to greenwash their people\",\n",
        "    \"terraform\":\"If we terraform another plants we have to make sure we build an equal society\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1tM0bd-bRAV4"
      },
      "outputs": [],
      "source": [
        "with open(path+\"new_verbs.json\",\"w\") as f:\n",
        "    json.dump(sentences,f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4CInsLWc7VaP"
      },
      "source": [
        "# Arguments classification test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "RscMahMY7VaP"
      },
      "outputs": [],
      "source": [
        "#All the sentences generated will be stred in this folder:\n",
        "path=\"./Data/Argument_classification/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "editor = Editor()\n",
        "verb=[\"went\"]\n",
        "ret = editor.template('{first_name} {verb} with {first_name2} to the {mask}', verb=verb,nsamples=100)\n",
        "tags=['B-ARG0',\n",
        "    'B-V',\n",
        "    'B-ARGM-COM',\n",
        "    'I-ARGM-COM',\n",
        "   'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
        "dataset={\"data\":ret.data,\"labels\":tags}     \n",
        "with open(path+\"FirstNames_sents.json\",'w') as f:\n",
        "    json.dump(dataset,f,indent=4) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "editor = Editor()\n",
        "pronoun_subj=['I','You','He','She',\"We\",\"They\"]\n",
        "pronoun_obj=[\"me\",\"you\",\"her\",\"him\",\"us\",\"them\"]\n",
        "verb=[ \"went\"]\n",
        "ret = editor.template('{psubj} {verb} with {pobj} to the {mask}',psubj=pronoun_subj,pobj=pronoun_obj, verb=verb,nsamples=100)\n",
        "tags=['B-ARG0',\n",
        "    'B-V',\n",
        "    'B-ARGM-COM',\n",
        "    'I-ARGM-COM',\n",
        "    'B-ARG4', 'I-ARG4', 'I-ARG4']\n",
        "dataset={\"data\":ret.data,\"labels\":tags}     \n",
        "with open(path+\"Pronouns_sents.json\",\"w\") as f:\n",
        "    json.dump(dataset,f,indent=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiwvQO-c7VaP"
      },
      "source": [
        "### Typos \n",
        "Here we create perturbed sentences with Checklist.\n",
        "It is possible to perturbate the sentences up to n times.\n",
        "From 1 to n typos dataset will be saved as well (to make a nice comparison)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "_mk314pDVAll"
      },
      "outputs": [],
      "source": [
        "def add_n_typos(sents,n):\n",
        "  \"\"\"\n",
        "  This functions perturb a sentence with n typos\n",
        "  Returns a dict like {original:Typos_sents}\n",
        "  \"\"\"\n",
        "  original=sents\n",
        "  for x in range(n):\n",
        "    sents=[Perturb.add_typos(x) for x in sents]\n",
        "  return dict(zip(original,sents))\n",
        "  \n",
        "def create_Multiple_typos_sentences(sents,n):\n",
        "  \"\"\"\n",
        "  This function creates n dictionaries with sentences perturbed n times where n goes from 1 to n\n",
        "  reutrns dict of dicts\n",
        "  \"\"\"\n",
        "  dict_typos={}\n",
        "  for i in range(1,n+1):\n",
        "    dict_typos[i]=add_n_typos(sents,i)\n",
        "  return dict_typos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "rPdDCXg27VaR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "editor = Editor()\n",
        "transitive_verb_list=transitive_verbs = ['build', 'buy', 'clean', 'cook', 'create', 'destroy', 'eat', 'fix', 'hold', 'paint', 'read', 'sell', 'send', 'show', 'teach', 'throw', 'use', 'watch', 'write']\n",
        "ret = editor.template('{first_name} and {first_name2} {tverb} {a:mask} {mask}.',tverb=transitive_verb_list, nsamples=100,keep_original=True)\n",
        "tags=['B-ARG0','I-ARG0','I-ARG0','B-V','B-ARG1','I-ARG1','I-ARG1','O']\n",
        "correct_sents=ret.data\n",
        "all_typos_sents=create_Multiple_typos_sentences(correct_sents,4)\n",
        "\n",
        "for i in (all_typos_sents.keys()):\n",
        "  print(i)\n",
        "  with open(path+f\"sents_{i}_typos.json\",\"w\") as f:\n",
        "    all_typos_sents[i][\"labels\"]=tags\n",
        "    json.dump(all_typos_sents[i],f)\n",
        "   "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_NER_sent():\n",
        "    golden_tags=\"['B-ARG0', 'I-ARG0', 'I-ARG0', 'B-V', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'B-ARGM-LOC', 'I-ARGM-LOC']\"\n",
        "    editor = Editor()\n",
        "    first1 = [x.split()[0] for x in editor.lexicons.male_from.Vietnam +  editor.lexicons.female_from.Vietnam]\n",
        "    first2 = [x.split()[0] for x in editor.lexicons.male_from.Indonesia +  editor.lexicons.female_from.Nepal]\n",
        "    last = [x.split()[0] for x in editor.lexicons.last_from.Cameroon + editor.lexicons.last_from.Palau]\n",
        "    cityy=[x for x in editor.lexicons.country_city.Ethiopia + editor.lexicons.country_city.Russia+editor.lexicons.country_city.South_Africa if len(x.split())==1] \n",
        "    t = editor.template(' {first_name} {first_name1} {last_name1} saw {first1_name2} {first1_name3} {last_name4} in {city}', first_name=first1,first1_name=first2, last_name=last, city=cityy,meta=True, nsamples=100)\n",
        "    return {golden_tags:t.data}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "di=create_NER_sent()\n",
        "with open(path+\"NER_sentences.json\",\"w\") as f:\n",
        "    json.dump(di,f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Those are the keys of luis'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"It annoys me the results of the election\"\n",
        "\"Those are the keys of luis\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PP attachment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[\"I fixed the car with a red logo\",\"I fixed the car with a wretch\"]\n",
        "[\"I bought a computer with GPU\",\" I bought a computer wit bitcoins\"]\n",
        "[\"I went to the resturant by the Hutsin\", \"I went to the resturant by bike\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlpTaskKernel",
      "language": "python",
      "name": "nlptaskkernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
