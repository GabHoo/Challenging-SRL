{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ35qPko7VZ7"
      },
      "source": [
        "# This notebook containes all the functions and manually created sentences to further generate (Automatically or semi-automatically) the test sets used in *Challenging-SRL*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjmZ36D27VaA"
      },
      "outputs": [],
      "source": [
        "pip install checklist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lTp0y5ef7VZ-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "from checklist.editor import Editor\n",
        "from checklist.perturb import Perturb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35lOyg867VaB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhnXhIPo7VaC"
      },
      "source": [
        "## Predicate identification test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "jQzOoxEC7VaC"
      },
      "outputs": [],
      "source": [
        "#All the sentences generated will be stred in this folder:\n",
        "path=\"./Data/Predicate_identification/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5nYfnyP7VaD"
      },
      "source": [
        "### Generate Data for contraction VERB identification.\n",
        "\n",
        "Creation semi manual of the contraction data test.  \n",
        "Some sentences with predicates that can be subjected to this phenomena are written by the author and Checklist will contract or expand the sentence.  \n",
        "The list is not exhaustive and can be always be expanded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKkINuvI7VaE"
      },
      "outputs": [],
      "source": [
        "def create_contractions(data):\n",
        "    \"\"\"This function creates contractions from a list of sentences.\n",
        "    Either contrated or expanded. Using Checklist Perturbation.contractions\n",
        "\n",
        "    :data is a list of sententes\n",
        "\n",
        "    Returns a list of nested list [sent,sent]\n",
        "    \"\"\"\n",
        "\n",
        "    ret = Perturb.perturb(data, Perturb.contractions)\n",
        "\n",
        "\n",
        "    return ret.data\n",
        "\n",
        "def create_dictionary_Format(data,sents):\n",
        "    \"\"\" \n",
        "    This function is exclusively for formatting a new dictionary.\n",
        "    \n",
        "    :data is {sent:verb_indx}\n",
        "    :sents is list of coupled lists [contracted , expanded]\n",
        "\n",
        "    Output: A nested dictionary\n",
        "    {sentence:(contracted_sentence:verb_indx)}\n",
        "    \"\"\"\n",
        "    co=np.array(sents).T[1]\n",
        "    new_dict = {s: x for s,x in zip(co,data.items())}\n",
        "    return new_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL-BBU1m7VaF"
      },
      "outputs": [],
      "source": [
        "#THIS sentences were manually generated taking inspiration from Checklist perturbator.py where a various contractions are listed\n",
        "\n",
        "data = {'it\\'s a wonderfull day': 1,\n",
        "        \"where did he go?\":1,\n",
        "        \"There's some pesto left\": 1,\n",
        "        \"He was ought not to do it\": 2,\n",
        "        \"I could've tried that as well\": 2,\n",
        "        \"They will leave the house\": 1,\n",
        "        \"That would be creazy\": 1,\n",
        "        \"we are here\": 1,\n",
        "        \"Mark had not see that coming\": 1,\n",
        "        \"She will be a great candidate\": 1,\n",
        "        \"Mary is not a nurse.\": 1,\n",
        "        \"He's gone already\": 1,\n",
        "        \"I would like some tea\": 1,\n",
        "        \"who is there?\": 1,\n",
        "        \"I could not eat some food now\": 1,\n",
        "        \"We've decided to change house\": 1,\n",
        "        \"I must not lose my temper\": 1,\n",
        "        \"You might not want to do that\": 1\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqemIl1x7VaH",
        "outputId": "0f9d58eb-e0d0-4b24-c6dc-3011c3082187"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "({'it is a wonderfull day': (\"it's a wonderfull day\", 1),\n",
              "  \"where'd he go?\": ('where did he go?', 1),\n",
              "  'There is some pesto left': (\"There's some pesto left\", 1),\n",
              "  \"He was oughtn't to do it\": ('He was ought not to do it', 2),\n",
              "  'I could have tried that as well': (\"I could've tried that as well\", 2),\n",
              "  \"They'll leave the house\": ('They will leave the house', 1),\n",
              "  \"That'd be creazy\": ('That would be creazy', 1),\n",
              "  \"we're here\": ('we are here', 1),\n",
              "  \"Mark hadn't see that coming\": ('Mark had not see that coming', 1),\n",
              "  \"She'll be a great candidate\": ('She will be a great candidate', 1),\n",
              "  \"Mary isn't a nurse.\": ('Mary is not a nurse.', 1),\n",
              "  'He is gone already': (\"He's gone already\", 1),\n",
              "  \"I'd like some tea\": ('I would like some tea', 1),\n",
              "  \"who's there?\": ('who is there?', 1),\n",
              "  \"I couldn't eat some food now\": ('I could not eat some food now', 1),\n",
              "  'We have decided to change house': (\"We've decided to change house\", 1),\n",
              "  \"I mustn't lose my temper\": ('I must not lose my temper', 1),\n",
              "  \"You mightn't want to do that\": ('You might not want to do that', 1)},\n",
              " 18)"
            ]
          },
          "execution_count": 235,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sents=create_contractions(list(data.keys()))\n",
        "out_dict=create_dictionary_Format(data,sents)\n",
        "out_dict,len(out_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTi7AYs67VaI"
      },
      "outputs": [],
      "source": [
        "with open(path+\"contracted_sentences.json\",\"w\") as f:\n",
        "    json.dump(out_dict,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9wUvZw_7VaI"
      },
      "source": [
        "### Predicate irregular inflections\n",
        "Semi automatic creation of sentences using a list of irregular inflected verb.  \n",
        "List was found online / written by the authros. Example sentences are contructed with Large Language Model RoBerta integrated in CheckList library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5jEOMl97VaJ"
      },
      "outputs": [],
      "source": [
        "def create_inflected_sentences(irregular_inflections):\n",
        "    \"\"\"This function creates a dictionary in the shape {label:sentence}. Where label is the inflected verb form.\n",
        "    :irregular_inflections is a list of the irregular verbs to be used.\n",
        "    returns Dict\n",
        "    \"\"\"\n",
        "    editor = Editor()\n",
        "    #This will add the irregular verbs to the lexicon so that we can use them in the template.\n",
        "    editor.add_lexicon('irr_verb', irregular_inflections,remove_duplicates=True)\n",
        "\n",
        "    #This will create 1000 samples sentences and their lables will be the irregular verb picked.\n",
        "    #{mask} is a special token that will be replaced by a random word suggested by the Language model.\n",
        "    #{fist_name} is a special token that will be replaced by a random first name in the lexicon.\n",
        "    ret = editor.template('{first_name} {irr_verb} {a:mask} {mask}.',nsamples=100,labels='{irr_verb}')\n",
        "\n",
        "    #This creates a dictionary in the shape {label:sentence}. Where label is the verb.\n",
        "    #Carefull, this will be much smaller than the number of samples because some of the sentences will be duplicates.\n",
        "    inflected_sentences=dict(zip(ret.labels,ret.data))\n",
        "\n",
        "\n",
        "    return inflected_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXQMkv8I7VaK"
      },
      "outputs": [],
      "source": [
        "irregular_inflections=['Beheld', 'Dwelt', 'Flung', 'Broadcast', 'Clung', 'Dared', 'Fitted', 'Forgave', 'Grinded', 'Hanged', 'Knelt', 'Laid', 'Led', 'Leant', 'Molten', 'Mistook', 'Proved', 'Rose', 'Sawn', 'Sought', 'Sewed', 'Shaven', 'Slit', 'Snuck', 'Span', 'Spoiled', 'Spring', 'Stuck', 'Strode', 'Struck', 'Swung', 'Torn', 'Undertook', 'Vext', 'Wet', 'Wrote']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4FD531j7VaL",
        "outputId": "c6b25a8c-d894-4a2e-c841-7480a280bc27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(35,\n",
              " {'Leant': 'Christopher Leant a H __.',\n",
              "  'Knelt': 'Elizabeth Knelt an Old Woman.',\n",
              "  'Snuck': 'Katherine Snuck an Old Swan.',\n",
              "  'Rose': 'Al Rose a L Key.',\n",
              "  'Forgave': 'Al Forgave a New Thing.',\n",
              "  'Span': 'Joan Span a New Order.',\n",
              "  'Strode': 'Lauren Strode a L Stand.',\n",
              "  'Sewed': 'Christopher Sewed a L 3.',\n",
              "  'Undertook': 'Steve Undertook an Old Rose.',\n",
              "  'Led': 'Peter Led a H in.',\n",
              "  'Dwelt': 'Frances Dwelt a L No.',\n",
              "  'Swung': 'Paul Swung a H Band.',\n",
              "  'Sawn': 'Fiona Sawn an Old Girl.',\n",
              "  'Vext': 'Gary Vext an A Rose.',\n",
              "  'Flung': 'Frederick Flung a H Bar.',\n",
              "  'Wet': 'Donald Wet a R Road.',\n",
              "  'Slit': 'Catherine Slit a H Ranch.',\n",
              "  'Fitted': 'Henry Fitted a L 0.',\n",
              "  'Mistook': 'Emily Mistook an In A.',\n",
              "  'Dared': 'Jennifer Dared an Old Friend.',\n",
              "  'Spoiled': 'Al Spoiled a L Stand.',\n",
              "  'Shaven': 'Sue Shaven a H in.',\n",
              "  'Sought': 'Martha Sought a L Ring.',\n",
              "  'Molten': 'Bill Molten an Old Thing.',\n",
              "  'Hanged': 'Susan Hanged an Old Car.',\n",
              "  'Stuck': 'Frank Stuck a R Post.',\n",
              "  'Proved': 'Simon Proved a L H.',\n",
              "  'Broadcast': 'Mark Broadcast a L Stand.',\n",
              "  'Beheld': 'Sarah Beheld an Old Rose.',\n",
              "  'Struck': 'Kathy Struck a New Frontier.',\n",
              "  'Laid': 'Christine Laid a L K.',\n",
              "  'Wrote': 'Frances Wrote an Old Chair.',\n",
              "  'Torn': 'Michelle Torn a New Order.',\n",
              "  'Grinded': 'Jay Grinded an A Lot.',\n",
              "  'Spring': 'Sarah Spring a L 3.'})"
            ]
          },
          "execution_count": 241,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inflected_sentences=create_inflected_sentences(irregular_inflections)\n",
        "len(inflected_sentences),inflected_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "SAgD4ute7VaM",
        "outputId": "a3737198-71b4-4529-a63d-0ee68c22b611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-e7772469ab27>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"inflected_sentences.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minflected_sentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Data/Predicate_identification/inflected_sentences.json'"
          ]
        }
      ],
      "source": [
        "with open(path+\"inflected_sentences.json\",\"w\") as f:\n",
        "    json.dump(inflected_sentences,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwLudL0h7VaM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ozk3dcz7VaN"
      },
      "source": [
        "# Typos\n",
        "For the creation of the this dataset we take a list of verbs (possibly transitive) from ChatGPT/the internet. For each verb we add a typo by switching two characters with the help of Checklist perturbator class. We then use Checklist template to fill a tamplate with the perturbated verb."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_verb_typos(verb_list):\n",
        "  \"\"\"This function creates a list of sentences with a perturbate verbs (typos).\n",
        "  It first generated  the list of wrong verbs from the input list and then create as many sentences.\n",
        "  \"\"\"\n",
        "  editor=Editor()\n",
        "  verb_typos=[Perturb.add_typos(x) for x in verb_list]\n",
        "\n",
        "  editor.add_lexicon('verb_typos', verb_typos,remove_duplicates=True)\n",
        "  editor.add_lexicon('adj', ['good', 'bad', 'great', 'terrible','wierd','cool','aweful'])\n",
        "\n",
        "  ret = editor.template('They {verb_typos} a {adj} {mask}.',nsamples=len(verb_list), remove_duplicates=True,labels='{verb_typos}')\n",
        "  return dict(zip(ret.labels, ret.data))\n"
      ],
      "metadata": {
        "id": "U3pQ8jrg89_G"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "rBF1ig_w7VaN"
      },
      "outputs": [],
      "source": [
        "verb_list=['Beheld', 'Flung', 'Broadcast', 'Forgave', 'Grinded', 'Hanged', 'Laid', 'Led', 'Leant', 'Molten', 'Mistook', 'Proved', 'Sawn', 'Sought', 'Sewed', 'Shaven', 'Slit', 'Snuck', 'Span', 'Spoiled', 'Stuck', 'Strode', 'Struck', 'Swung', 'Torn', 'Undertook', 'Vext', 'Wet', 'Wrote','eat', 'drink', 'throw', 'catch', 'write', 'read', 'hit', 'kick', 'open', 'close', 'cook', 'bake', 'paint', 'draw', 'build', 'break', 'repair', 'clean', 'wash', 'drive', 'ride', 'carry', 'lift', 'play', 'sing', 'dance', 'love', 'hate', 'need', 'want', 'like', 'dislike', 'teach', 'learn', 'understand', 'know', 'remember', 'forget', 'help', 'hurt', 'show']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sents=create_verb_typos(verb_list)\n",
        "#sents"
      ],
      "metadata": {
        "id": "jpLyQs6aCpZL"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path+\"verb_typos_sentence.json\",\"w\") as f:\n",
        "    json.dump(inflected_sentences,f)"
      ],
      "metadata": {
        "id": "0w6h-gg7HISM",
        "outputId": "5cc17cce-d588-489e-976c-2a337c4972bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-9658a42ec600>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"verb_typos_sentence.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minflected_sentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CInsLWc7VaP"
      },
      "source": [
        "# ARGOUMENTS CLASSIFICATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RscMahMY7VaP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR1zKEO77VaP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiwvQO-c7VaP"
      },
      "source": [
        "## ROBUSTNESS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEFg_GAL7VaQ",
        "outputId": "90580413-abea-4f3a-cb4c-c18af21d8779"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gabhoo/miniconda3/envs/nlptasks/lib/python3.10/site-packages/checklist/text_generation.py:171: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:204.)\n",
            "  to_pred = torch.tensor(to_pred, device=self.device).to(torch.int64)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MunchWithAdd({'data': ['Jason saw an empty room.', 'Alice saw an innocent child.', 'Jerry saw an imminent threat.', 'Paul saw an old man.', 'Deborah saw an empty building.']})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "editor = Editor()\n",
        "#This will add the irregular verbs to the lexicon so that we can use them in the template.\n",
        "#editor.add_lexicon('irr_verb', irregular_inflections,remove_duplicates=True)\n",
        "\n",
        "#This will create 1000 samples sentences and their lables will be the irregular verb picked.\n",
        "#{mask} is a special token that will be replaced by a random word suggested by the Language model.\n",
        "#{fist_name} is a special token that will be replaced by a random first name in the lexicon.\n",
        "ret = editor.template('{first_name} saw {a:mask} {mask}.',nsamples=5,keep_original=True)\n",
        "\n",
        "#This creates a dictionary in the shape {label:sentence}. Where label is the verb.\n",
        "#Carefull, this will be much smaller than the number of samples because some of the sentences will be duplicates.\n",
        "#inflected_sentences=dict(zip(ret.labels,ret.data))\n",
        "ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPdDCXg27VaR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlpTaskKernel",
      "language": "python",
      "name": "nlptaskkernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}